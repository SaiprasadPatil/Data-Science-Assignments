{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3dea429-de2f-4642-91cc-8855768c62ea",
   "metadata": {},
   "source": [
    "### Q1. Mathematical Formula for a Linear SVM\n",
    "\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) involves finding the optimal hyperplane that separates the data into different classes with the maximum margin. The decision function of a linear SVM can be written as:\n",
    "\n",
    "\\[ f(x) = \\mathbf{w}^T \\mathbf{x} + b \\]\n",
    "\n",
    "where:\n",
    "- \\(\\mathbf{w}\\) is the weight vector (normal to the hyperplane),\n",
    "- \\(\\mathbf{x}\\) is the feature vector,\n",
    "- \\(b\\) is the bias term.\n",
    "\n",
    "The hyperplane is defined by the equation:\n",
    "\n",
    "\\[ \\mathbf{w}^T \\mathbf{x} + b = 0 \\]\n",
    "\n",
    "### Q2. Objective Function of a Linear SVM\n",
    "\n",
    "The objective of a linear SVM is to maximize the margin between the two classes while minimizing the classification error. The objective function of a linear SVM can be formulated as:\n",
    "\n",
    "**Maximize**: \\(\\frac{2}{\\|\\mathbf{w}\\|}\\)\n",
    "\n",
    "This can be rewritten as minimizing the following cost function with regularization term:\n",
    "\n",
    "**Minimize**: \\(\\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i\\)\n",
    "\n",
    "where:\n",
    "- \\(\\|\\mathbf{w}\\|^2\\) is the squared norm of the weight vector,\n",
    "- \\(C\\) is the regularization parameter,\n",
    "- \\(\\xi_i\\) are the slack variables that allow for misclassification.\n",
    "\n",
    "The constraint for each data point \\( (\\mathbf{x_i}, y_i) \\) is:\n",
    "\n",
    "\\[ y_i (\\mathbf{w}^T \\mathbf{x_i} + b) \\geq 1 - \\xi_i \\]\n",
    "\\[ \\xi_i \\geq 0 \\]\n",
    "\n",
    "### Q3. Kernel Trick in SVM\n",
    "\n",
    "The kernel trick is a technique used in SVM to handle non-linearly separable data. It allows SVM to operate in a higher-dimensional space without explicitly transforming the data into that space. This is done by applying a kernel function \\(K(\\mathbf{x_i}, \\mathbf{x_j})\\) that computes the inner product in the higher-dimensional feature space:\n",
    "\n",
    "\\[ K(\\mathbf{x_i}, \\mathbf{x_j}) = \\phi(\\mathbf{x_i})^T \\phi(\\mathbf{x_j}) \\]\n",
    "\n",
    "where \\(\\phi(\\cdot)\\) is a mapping function to the higher-dimensional space. Common kernel functions include:\n",
    "\n",
    "- **Linear Kernel**: \\(K(\\mathbf{x_i}, \\mathbf{x_j}) = \\mathbf{x_i}^T \\mathbf{x_j}\\)\n",
    "- **Polynomial Kernel**: \\(K(\\mathbf{x_i}, \\mathbf{x_j}) = (\\mathbf{x_i}^T \\mathbf{x_j} + c)^d\\)\n",
    "- **Radial Basis Function (RBF) Kernel**: \\(K(\\mathbf{x_i}, \\mathbf{x_j}) = \\exp\\left(-\\frac{\\|\\mathbf{x_i} - \\mathbf{x_j}\\|^2}{2\\sigma^2}\\right)\\)\n",
    "\n",
    "### Q4. Role of Support Vectors in SVM\n",
    "\n",
    "Support vectors are the data points that lie closest to the hyperplane and are critical in defining the position and orientation of the hyperplane. They are the points that contribute to the margin calculation and are the only points that affect the decision boundary of the SVM. \n",
    "\n",
    "**Example:**\n",
    "\n",
    "If you have a dataset with two classes, the support vectors will be the points from each class that are closest to the decision boundary. If you remove these points, the position of the decision boundary might change.\n",
    "\n",
    "### Q5. Illustrations: Hyperplane, Marginal Plane, Soft Margin, and Hard Margin\n",
    "\n",
    "**1. Hyperplane:** The decision boundary that separates the two classes. In 2D, it's a line; in 3D, it's a plane.\n",
    "\n",
    "**2. Marginal Plane:** The planes parallel to the hyperplane that are at the distance of the margin from the hyperplane. They pass through the support vectors.\n",
    "\n",
    "**3. Hard Margin:** Used when the data is linearly separable, and there is no allowance for misclassification. The margin is maximized, and no data points are within the margin.\n",
    "\n",
    "**4. Soft Margin:** Allows some misclassification by introducing slack variables. It balances the trade-off between maximizing the margin and minimizing the classification error.\n",
    "\n",
    "**Visualizations:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Use only two classes for binary classification example\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM with linear kernel\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot decision boundary\n",
    "def plot_decision_boundary(clf, X, y, ax):\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.8)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=50, cmap=plt.cm.coolwarm)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_decision_boundary(clf, X_train, y_train, ax)\n",
    "plt.title('Decision Boundary with Hard Margin')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "For soft margin, use `C` values to adjust the regularization:\n",
    "\n",
    "```python\n",
    "# Train SVM with soft margin\n",
    "clf_soft = SVC(kernel='linear', C=0.1)\n",
    "clf_soft.fit(X_train, y_train)\n",
    "\n",
    "# Plot decision boundary for soft margin\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_decision_boundary(clf_soft, X_train, y_train, ax)\n",
    "plt.title('Decision Boundary with Soft Margin')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Q6. SVM Implementation on Iris Dataset\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "```\n",
    "\n",
    "### Bonus Task: Implement Linear SVM from Scratch\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.001, epochs=1000, C=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.C = C\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.weights) + self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights - np.dot(x_i, y[idx]))\n",
    "                    self.bias -= self.learning_rate * y[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Use only two classes for binary classification example\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the\n",
    "\n",
    " Linear SVM\n",
    "linear_svm = LinearSVM()\n",
    "linear_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = linear_svm.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of custom SVM: {accuracy:.2f}')\n",
    "\n",
    "# Compare with scikit-learn SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train scikit-learn linear SVM\n",
    "sklearn_svm = SVC(kernel='linear', C=1.0)\n",
    "sklearn_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_sklearn = sklearn_svm.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f'Accuracy of scikit-learn SVM: {accuracy_sklearn:.2f}')\n",
    "\n",
    "# Plot decision boundaries of the custom linear SVM\n",
    "def plot_decision_boundary(model, X, y, ax):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.8)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=50, cmap=plt.cm.coolwarm)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_decision_boundary(linear_svm, X_train, y_train, ax)\n",
    "plt.title('Decision Boundary of Custom Linear SVM')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "This guide covers the mathematical foundation and implementation details of Support Vector Machines (SVM), including the objective function, kernel trick, and role of support vectors. It also illustrates concepts like hyperplanes, margins, and the difference between hard and soft margins. The final part involves implementing a linear SVM from scratch, comparing its performance with scikit-learn's SVM, and visualizing decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8541d-4313-44bb-8aa0-352a4d36b3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b718c3-4429-41a8-8555-4e385504b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
