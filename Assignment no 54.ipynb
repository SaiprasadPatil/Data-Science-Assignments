{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94ef85a-63a3-4d6e-916d-14105dcb6bb7",
   "metadata": {},
   "source": [
    "Hereâ€™s a detailed breakdown of the questions related to Support Vector Machines (SVMs) and Support Vector Regression (SVR), as well as an assignment for implementing and tuning an SVM classifier.\n",
    "\n",
    "### Q1. Relationship Between Polynomial Functions and Kernel Functions\n",
    "\n",
    "In machine learning, polynomial functions and kernel functions are closely related. The kernel trick allows algorithms like SVM to operate in a higher-dimensional space without explicitly computing the coordinates in that space. \n",
    "\n",
    "- **Polynomial Functions**: Polynomial functions can map data into a higher-dimensional space where linear separability might be easier. For example, a polynomial function of degree 2 can transform a 2D dataset into a 3D dataset.\n",
    "\n",
    "- **Kernel Functions**: A polynomial kernel is a type of kernel function that computes the dot product of the input features in the higher-dimensional space implicitly. It applies the polynomial function to the dot product of the original features.\n",
    "\n",
    "The polynomial kernel function is defined as:\n",
    "\n",
    "\\[ K(\\mathbf{x_i}, \\mathbf{x_j}) = (\\mathbf{x_i}^T \\mathbf{x_j} + c)^d \\]\n",
    "\n",
    "where:\n",
    "- \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_j}\\) are input feature vectors,\n",
    "- \\(c\\) is a constant term,\n",
    "- \\(d\\) is the degree of the polynomial.\n",
    "\n",
    "This kernel function allows SVM to create non-linear decision boundaries by implicitly mapping the input data into a higher-dimensional space.\n",
    "\n",
    "### Q2. Implementing SVM with a Polynomial Kernel in Python Using Scikit-Learn\n",
    "\n",
    "Here's how you can implement an SVM with a polynomial kernel using Scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Use only two classes for simplicity\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the SVC classifier with a polynomial kernel\n",
    "poly_svc = SVC(kernel='poly', degree=3, C=1.0, gamma='auto')\n",
    "poly_svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the testing data\n",
    "y_pred = poly_svc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "```\n",
    "\n",
    "### Q3. Effect of Epsilon on the Number of Support Vectors in SVR\n",
    "\n",
    "In Support Vector Regression (SVR), the epsilon parameter (\\(\\epsilon\\)) defines a margin of tolerance where no penalty is given for errors. It affects the number of support vectors as follows:\n",
    "\n",
    "- **Higher Epsilon Value**: A larger \\(\\epsilon\\) means a larger margin of tolerance. This generally results in fewer support vectors because the model is less strict about fitting the data points within the margin, leading to a simpler model.\n",
    "\n",
    "- **Lower Epsilon Value**: A smaller \\(\\epsilon\\) makes the margin narrower, meaning the model will pay more attention to fitting the training data precisely. This typically increases the number of support vectors as more data points are considered within the margin.\n",
    "\n",
    "### Q4. Impact of Kernel Function, C, Epsilon, and Gamma on SVR Performance\n",
    "\n",
    "**1. Kernel Function**: \n",
    "- **Linear Kernel**: Suitable for linearly separable data.\n",
    "- **Polynomial Kernel**: Can model non-linear relationships by increasing the dimensionality implicitly.\n",
    "- **RBF Kernel**: Effective for capturing complex relationships by mapping data into an infinite-dimensional space.\n",
    "\n",
    "**2. C Parameter**:\n",
    "- **Higher C**: Encourages the model to fit the training data more accurately, which can lead to overfitting, especially with noise in the data.\n",
    "- **Lower C**: Increases the margin and allows for more errors on the training data, which can improve generalization but may underfit.\n",
    "\n",
    "**3. Epsilon Parameter (\\(\\epsilon\\))**:\n",
    "- **Higher \\(\\epsilon\\)**: Allows for a larger margin of tolerance, reducing the number of support vectors and possibly simplifying the model.\n",
    "- **Lower \\(\\epsilon\\)**: Results in a smaller margin of tolerance, making the model more sensitive to training data and potentially increasing the number of support vectors.\n",
    "\n",
    "**4. Gamma Parameter (\\(\\gamma\\)) in RBF Kernel:\n",
    "- **Higher \\(\\gamma\\)**: Leads to a more complex decision boundary that closely fits the training data, which can cause overfitting.\n",
    "- **Lower \\(\\gamma\\)**: Creates a smoother decision boundary that may generalize better but might underfit if too simple.\n",
    "\n",
    "### Q5. Assignment\n",
    "\n",
    "Here's a step-by-step guide to complete the assignment:\n",
    "\n",
    "**1. Import Libraries and Load Dataset**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://drive.google.com/uc?id=1Q4J8KS1wm4-_YTuc389enPh6O-eTNcx2'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "```\n",
    "\n",
    "**2. Split the Dataset**\n",
    "\n",
    "```python\n",
    "# Define features and target variable\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "**3. Preprocess the Data**\n",
    "\n",
    "```python\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "**4. Create and Train the SVC Classifier**\n",
    "\n",
    "```python\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**5. Predict and Evaluate**\n",
    "\n",
    "```python\n",
    "# Predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "```\n",
    "\n",
    "**6. Hyperparameter Tuning**\n",
    "\n",
    "```python\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters and best score\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best Score: {grid_search.best_score_:.2f}')\n",
    "```\n",
    "\n",
    "**7. Train the Tuned Classifier**\n",
    "\n",
    "```python\n",
    "# Train the classifier with the best parameters on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X, y)\n",
    "```\n",
    "\n",
    "**8. Save the Trained Classifier**\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(best_svc, 'best_svc_model.pkl')\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Polynomial Kernel**: Maps data to higher-dimensional space to handle non-linear relationships.\n",
    "- **SVR Parameters**: Epsilon affects the number of support vectors; C, epsilon, gamma, and kernel type all affect the model's performance.\n",
    "- **Implementation**: The assignment involves preprocessing data, training an SVM model, evaluating performance, tuning hyperparameters, and saving the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143b427-ed5c-42d8-8387-ed90863c5c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b71b16a-a147-46af-bc69-ea3ed10c0862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
