{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b998b68-6549-4001-992d-d936e0ca52b8",
   "metadata": {},
   "source": [
    "Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with\n",
    "an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d2f17-83e5-4ab1-b0d9-c1ea79c9b1d2",
   "metadata": {},
   "source": [
    "The Probability Mass Function (PMF) and Probability Density Function (PDF) are concepts used in probability theory and statistics to describe the likelihood of different outcomes in discrete and continuous random variables, respectively.\n",
    "\n",
    "### Probability Mass Function (PMF):\n",
    "The PMF is a function that gives the probability that a discrete random variable is equal to a specific value. In other words, it maps each possible value of the random variable to its probability of occurrence.\n",
    "\n",
    "Mathematically, for a discrete random variable \\( X \\), the PMF is denoted as \\( p(x) \\), where \\( p(x) \\) represents the probability that \\( X \\) takes on the value \\( x \\). The PMF satisfies the following properties:\n",
    "1. \\( 0 \\leq p(x) \\leq 1 \\) for all \\( x \\) in the domain of \\( X \\).\n",
    "2. The sum of probabilities over all possible values of \\( X \\) is equal to 1: \\( \\sum_{\\text{all } x} p(x) = 1 \\).\n",
    "\n",
    "### Example of PMF:\n",
    "Consider rolling a fair six-sided die. The random variable \\( X \\) represents the outcome of the roll. The PMF of \\( X \\) would assign probabilities to each possible outcome from 1 to 6. Since the die is fair, each outcome has an equal probability of \\( \\frac{1}{6} \\). Therefore, the PMF would be:\n",
    "\\[ p(x) = \\begin{cases} \\frac{1}{6}, & \\text{if } x \\in \\{1, 2, 3, 4, 5, 6\\} \\\\ 0, & \\text{otherwise} \\end{cases} \\]\n",
    "\n",
    "### Probability Density Function (PDF):\n",
    "The PDF is a function that describes the relative likelihood of different outcomes for a continuous random variable. Unlike the PMF, which gives the probability of specific values, the PDF gives the probability density (likelihood per unit interval) at each point in the domain of the random variable.\n",
    "\n",
    "Mathematically, for a continuous random variable \\( X \\), the PDF is denoted as \\( f(x) \\), where \\( f(x) \\) represents the probability density at the point \\( x \\). The PDF satisfies the following properties:\n",
    "1. \\( f(x) \\geq 0 \\) for all \\( x \\).\n",
    "2. The area under the curve of \\( f(x) \\) over any interval gives the probability that \\( X \\) lies within that interval: \\( \\int_a^b f(x) \\, dx = P(a \\leq X \\leq b) \\).\n",
    "\n",
    "### Example of PDF:\n",
    "Consider the standard normal distribution, which has a bell-shaped curve. The PDF of the standard normal distribution is given by the formula:\n",
    "\\[ f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} \\]\n",
    "This function describes the relative likelihood of different values of \\( x \\) in the standard normal distribution.\n",
    "\n",
    "In summary, while the PMF is used for discrete random variables to give the probability of specific values, the PDF is used for continuous random variables to describe the relative likelihood of different outcomes across the entire range of the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900e689-a49e-4b5a-abb5-0ce91c4c9af8",
   "metadata": {},
   "source": [
    "Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3fd3a-864c-4bb7-8479-f20fa63f0f8d",
   "metadata": {},
   "source": [
    "### Mathematically:\n",
    "For a random variable \\( X \\), the CDF is denoted as \\( F(x) \\), where \\( F(x) \\) represents the probability that \\( X \\) is less than or equal to \\( x \\). Mathematically, it is defined as:\n",
    "\\[ F(x) = P(X \\leq x) \\]\n",
    "\n",
    "### Properties:\n",
    "1. \\( 0 \\leq F(x) \\leq 1 \\) for all \\( x \\).\n",
    "2. \\( F(x) \\) is non-decreasing: \\( F(x_1) \\leq F(x_2) \\) if \\( x_1 \\leq x_2 \\).\n",
    "3. \\( \\lim_{x \\to -\\infty} F(x) = 0 \\) and \\( \\lim_{x \\to \\infty} F(x) = 1 \\).\n",
    "\n",
    "### Example of CDF:\n",
    "Consider rolling a fair six-sided die. Let \\( X \\) be the random variable representing the outcome of the roll. The CDF of \\( X \\) would give the probability that the outcome is less than or equal to a certain value. Since the die is fair, the CDF would increase in a step-wise manner, with jumps of \\( \\frac{1}{6} \\) at each integer value from 1 to 6.\n",
    "\n",
    "\\[ F(x) = \\begin{cases} 0, & \\text{if } x < 1 \\\\ \\frac{1}{6}, & \\text{if } 1 \\leq x < 2 \\\\ \\frac{2}{6}, & \\text{if } 2 \\leq x < 3 \\\\ \\frac{3}{6}, & \\text{if } 3 \\leq x < 4 \\\\ \\frac{4}{6}, & \\text{if } 4 \\leq x < 5 \\\\ \\frac{5}{6}, & \\text{if } 5 \\leq x < 6 \\\\ 1, & \\text{if } x \\geq 6 \\end{cases} \\]\n",
    "\n",
    "### Importance of CDF:\n",
    "1. **Probability Calculation**: The CDF provides an easy way to calculate the probability that a random variable falls within a certain range by subtracting the CDF values at the endpoints of the range.\n",
    "2. **Comparison of Distributions**: It allows for the comparison of different probability distributions by examining their cumulative probabilities.\n",
    "3. **Inference**: In statistical inference, the CDF is used to compute critical values and p-values for hypothesis testing.\n",
    "4. **Simulation and Modeling**: CDFs are used in simulation studies and modeling to generate random variates from a given distribution.\n",
    "\n",
    "In summary, the Cumulative Density Function (CDF) is a fundamental concept in probability theory and statistics, providing a way to describe and analyze the probabilities associated with random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705deb07-3c6d-495f-a23f-99c0fb75a0cf",
   "metadata": {},
   "source": [
    "Q3: What are some examples of situations where the normal distribution might be used as a model?\n",
    "Explain how the parameters of the normal distribution relate to the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffe9b8-74ce-469d-a9c4-e6de9f9234af",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution, is one of the most widely used probability distributions in various fields due to its many desirable properties. Some examples of situations where the normal distribution might be used as a model include:\n",
    "\n",
    "1. **Measurement Errors**: In many scientific experiments or observations, measurement errors often follow a normal distribution. For instance, errors in laboratory measurements, medical tests, or industrial processes can often be reasonably modeled using a normal distribution.\n",
    "\n",
    "2. **Biological Phenomena**: Many biological phenomena, such as heights, weights, blood pressure, and IQ scores in a population, are often approximately normally distributed. This makes the normal distribution a useful model in studies related to biology and medicine.\n",
    "\n",
    "3. **Financial Data**: In finance and economics, many variables such as stock returns, interest rates, and prices of commodities tend to follow a normal distribution (or closely approximate it). This makes the normal distribution a common choice for modeling financial data.\n",
    "\n",
    "4. **Quality Control**: In manufacturing and quality control processes, characteristics such as product dimensions, weight, or strength often follow a normal distribution. This allows practitioners to use the normal distribution to make predictions and set quality control thresholds.\n",
    "\n",
    "5. **Psychological Tests**: Scores on standardized psychological tests, such as IQ tests or personality assessments, are often assumed to follow a normal distribution in the population.\n",
    "\n",
    "The parameters of the normal distribution, namely the mean (\\( \\mu \\)) and the standard deviation (\\( \\sigma \\)), play crucial roles in defining the shape of the distribution:\n",
    "\n",
    "- **Mean (\\( \\mu \\))**: The mean represents the central tendency of the distribution and determines where the peak of the distribution is located along the horizontal axis. Shifting the mean to the right (increasing \\( \\mu \\)) shifts the entire distribution to the right, while shifting it to the left (decreasing \\( \\mu \\)) shifts the distribution to the left.\n",
    "\n",
    "- **Standard Deviation (\\( \\sigma \\))**: The standard deviation measures the spread or dispersion of the distribution. A larger standard deviation indicates that the data points are more spread out from the mean, resulting in a wider and flatter distribution. Conversely, a smaller standard deviation results in a narrower and taller distribution, with data points clustered closer to the mean.\n",
    "\n",
    "In summary, the normal distribution is a versatile model that is often used in situations where data tend to cluster around a central value with symmetrical tails. The mean and standard deviation of the normal distribution determine the central tendency and spread of the data, respectively, and play a key role in shaping the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c34dfb-2096-4b7a-a0c0-b5d15148c331",
   "metadata": {},
   "source": [
    "Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal\n",
    "Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047037a-d7d8-47f0-8017-bd4d166a6626",
   "metadata": {},
   "source": [
    "\n",
    "The normal distribution, also known as the Gaussian distribution, is of paramount importance in statistics, probability theory, and various fields of science, engineering, and social sciences due to several key reasons:\n",
    "\n",
    "Central Limit Theorem (CLT): The normal distribution arises naturally as a limiting distribution in many situations, especially when dealing with the sum or average of a large number of independent random variables. The CLT states that the sum (or average) of a large number of independent and identically distributed random variables will be approximately normally distributed, regardless of the distribution of the individual variables. This property makes the normal distribution fundamental in statistical inference, hypothesis testing, and estimation.\n",
    "\n",
    "Mathematical Simplicity: The normal distribution is mathematically well-behaved and characterized by only two parameters: the mean (\n",
    "μ) and the standard deviation (\n",
    "σ). This simplicity facilitates calculations and analytical derivations in statistical analysis and modeling.\n",
    "\n",
    "Statistical Inference: Many statistical methods and procedures, such as confidence intervals, hypothesis testing, and regression analysis, are based on assumptions of normality. When data follow a normal distribution, statistical analyses become more powerful and efficient.\n",
    "\n",
    "Predictive Modeling: The normal distribution serves as a foundation for many predictive models in various fields, including finance, economics, engineering, and environmental sciences. For example, financial models often assume that stock returns follow a normal distribution, allowing practitioners to make predictions and estimate risks.\n",
    "\n",
    "Quality Control: In manufacturing and quality control processes, characteristics such as product dimensions, weight, or strength often follow a normal distribution. Understanding and modeling these distributions are crucial for setting quality control thresholds, identifying defects, and improving processes.\n",
    "\n",
    "Examples of real-life phenomena that can be modeled using the normal distribution include:\n",
    "\n",
    "Height and Weight: In populations, human height and weight tend to follow approximately normal distributions, with most individuals clustered around the mean and fewer individuals at the extremes.\n",
    "\n",
    "IQ Scores: Intelligence quotient (IQ) scores in a population are often assumed to follow a normal distribution, with the majority of individuals clustered around the average IQ score.\n",
    "\n",
    "Exam Scores: Scores on standardized exams, such as the SAT or GRE, often approximate a normal distribution, with most students scoring around the mean and fewer students scoring at the extreme ends of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf669df-e1d2-4808-b95b-f2f45f632a62",
   "metadata": {},
   "source": [
    "Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli\n",
    "Distribution and Binomial Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620b42a7-8af7-44bf-9018-344a4b150ff3",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is a discrete probability distribution that describes the outcome of a single Bernoulli trial, which is an experiment with only two possible outcomes: success or failure. It is named after the Swiss mathematician Jacob Bernoulli. The Bernoulli distribution is characterized by a single parameter \\( p \\), which represents the probability of success on a single trial.\n",
    "\n",
    "### Probability Mass Function (PMF) of Bernoulli Distribution:\n",
    "The probability mass function (PMF) of the Bernoulli distribution is given by:\n",
    "\\[ P(X = x) = \\begin{cases} p, & \\text{if } x = 1 \\\\ 1-p, & \\text{if } x = 0 \\end{cases} \\]\n",
    "where \\( X \\) is the random variable representing the outcome of the Bernoulli trial, and \\( p \\) is the probability of success.\n",
    "\n",
    "### Example of Bernoulli Distribution:\n",
    "An example of a Bernoulli trial is flipping a fair coin, where success (1) could represent getting heads, and failure (0) could represent getting tails. The probability of success \\( p \\) in this case would be \\( 0.5 \\), assuming the coin is fair.\n",
    "\n",
    "### Difference between Bernoulli Distribution and Binomial Distribution:\n",
    "The main difference between the Bernoulli distribution and the binomial distribution lies in the number of trials involved:\n",
    "\n",
    "1. **Bernoulli Distribution**:\n",
    "   - Describes the outcome of a single Bernoulli trial, which has only two possible outcomes (success or failure).\n",
    "   - Characterized by a single parameter \\( p \\), representing the probability of success on a single trial.\n",
    "\n",
    "2. **Binomial Distribution**:\n",
    "   - Describes the number of successes in a fixed number of independent Bernoulli trials (repeated Bernoulli trials).\n",
    "   - Characterized by two parameters: \\( n \\), the number of trials, and \\( p \\), the probability of success on each trial.\n",
    "   - The binomial distribution is the sum of \\( n \\) independent and identically distributed Bernoulli random variables.\n",
    "\n",
    "### Example to Illustrate the Difference:\n",
    "Consider the following scenarios:\n",
    "- **Bernoulli Distribution**: A single toss of a fair coin, where the outcome of interest is getting heads (success) or tails (failure).\n",
    "- **Binomial Distribution**: Tossing the same fair coin 10 times and counting the number of heads obtained out of the 10 tosses. Here, we are interested in the number of successes (heads) out of multiple trials, which follows a binomial distribution with \\( n = 10 \\) and \\( p = 0.5 \\) (assuming the coin is fair).\n",
    "\n",
    "In summary, while both the Bernoulli distribution and the binomial distribution deal with outcomes of Bernoulli trials, the Bernoulli distribution describes the outcome of a single trial, while the binomial distribution describes the number of successes in a fixed number of trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb7cc3-10e6-4de3-a76e-639e0c2ce2cd",
   "metadata": {},
   "source": [
    "Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset\n",
    "is normally distributed, what is the probability that a randomly selected observation will be greater\n",
    "than 60? Use the appropriate formula and show your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f66effd-2d17-41b6-8b6a-5e30cf7f190c",
   "metadata": {},
   "source": [
    "To find the probability that a randomly selected observation from a normally distributed dataset with a mean of 50 and a standard deviation of 10 will be greater than 60, we need to calculate the area under the normal distribution curve to the right of 60. This can be done using the standard normal distribution table or by standardizing and using the cumulative distribution function (CDF) of the standard normal distribution.\n",
    "\n",
    "First, we standardize the value 60 using the formula for z-score:\n",
    "\\[ z = \\frac{x - \\mu}{\\sigma} \\]\n",
    "where:\n",
    "- \\( x = 60 \\) (the value we're interested in)\n",
    "- \\( \\mu = 50 \\) (mean of the dataset)\n",
    "- \\( \\sigma = 10 \\) (standard deviation of the dataset)\n",
    "\n",
    "Substituting the given values into the formula:\n",
    "\\[ z = \\frac{60 - 50}{10} = \\frac{10}{10} = 1 \\]\n",
    "\n",
    "Now, we find the probability that a randomly selected observation will be greater than 60 by calculating the area under the standard normal distribution curve to the right of \\( z = 1 \\). We can find this using a standard normal distribution table or by using statistical software.\n",
    "\n",
    "Using a standard normal distribution table, we find that the area to the right of \\( z = 1 \\) is approximately 0.1587.\n",
    "\n",
    "So, the probability that a randomly selected observation will be greater than 60 is approximately \\( 0.1587 \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa2575-f796-4a96-b0e3-114c16fcbdc0",
   "metadata": {},
   "source": [
    "Q7: Explain uniform Distribution with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1d102-a589-46a3-952b-7c3a0739aeb7",
   "metadata": {},
   "source": [
    "The uniform distribution is a probability distribution where all outcomes within a given range are equally likely. In other words, each value in the range has the same probability of occurring. This distribution is characterized by two parameters: the minimum value \\(a\\) and the maximum value \\(b\\). \n",
    "\n",
    "### Probability Density Function (PDF) of Uniform Distribution:\n",
    "The probability density function (PDF) of the uniform distribution is defined as:\n",
    "\\[ f(x) = \\frac{1}{b - a} \\]\n",
    "for \\(a \\leq x \\leq b\\), and \\(0\\) otherwise.\n",
    "\n",
    "### Example of Uniform Distribution:\n",
    "An example of a uniform distribution is the roll of a fair six-sided die. In this case, the outcomes are integers from 1 to 6, and each outcome has an equal probability of \\( \\frac{1}{6} \\). The uniform distribution is applicable because each outcome is equally likely to occur, and there is no bias towards any particular outcome.\n",
    "\n",
    "Another example is the random selection of a number between 0 and 1 from a continuous uniform distribution. In this case, any number within the range [0, 1] has an equal probability of being selected. This type of distribution is commonly used in simulations, random number generation, and probabilistic modeling.\n",
    "\n",
    "### Importance of Uniform Distribution:\n",
    "The uniform distribution serves as a simple and fair model for situations where all outcomes within a given range are equally likely. It is often used in various fields, including statistics, probability theory, and computer science, for generating random numbers, modeling certain types of phenomena, and as a reference distribution in statistical tests.\n",
    "\n",
    "In summary, the uniform distribution provides a basic and intuitive model for situations where all outcomes are equally probable within a specified range, making it a valuable tool in probability and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd932d-3b19-4495-b5a0-83e39ebef66d",
   "metadata": {},
   "source": [
    "Q8: What is the z score? State the importance of the z score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c3656-f798-409d-9562-1f37e7fbd78d",
   "metadata": {},
   "source": [
    "The z-score, also known as the standard score, is a measure of how many standard deviations a data point is away from the mean of a dataset. It is a dimensionless quantity that allows data points from different distributions to be compared and standardized onto a common scale.\n",
    "\n",
    "The formula to calculate the z-score for a data point \\( x \\) in a dataset with mean \\( \\mu \\) and standard deviation \\( \\sigma \\) is given by:\n",
    "\\[ z = \\frac{x - \\mu}{\\sigma} \\]\n",
    "\n",
    "### Importance of the z-score:\n",
    "1. **Standardization**: The z-score standardizes data, allowing for comparisons between different datasets that may have different scales or units. By converting data to z-scores, we can determine how unusual or typical a data point is relative to the distribution of the dataset.\n",
    "\n",
    "2. **Identifying Outliers**: Z-scores can be used to identify outliers or extreme values in a dataset. Data points with z-scores that fall far from the mean (e.g., beyond ±2 or ±3 standard deviations) may be considered outliers or anomalies.\n",
    "\n",
    "3. **Probability Calculation**: Z-scores are used to calculate probabilities and determine the likelihood of obtaining a certain value or range of values in a normal distribution. The standard normal distribution table, also known as the z-table, provides probabilities corresponding to different z-scores.\n",
    "\n",
    "4. **Hypothesis Testing**: Z-scores play a crucial role in hypothesis testing, where they are used to determine whether an observed difference between groups is statistically significant. Z-scores quantify the difference between a sample statistic (e.g., sample mean) and the population parameter (e.g., population mean) in terms of standard deviations.\n",
    "\n",
    "5. **Data Analysis and Visualization**: Z-scores are often used in data analysis and visualization to highlight patterns, trends, or deviations from expected values. Plotting z-scores can help visualize the relative position of data points within a distribution.\n",
    "\n",
    "Overall, the z-score is a valuable statistical tool that provides a standardized measure of a data point's relative position within a dataset, facilitating comparisons, interpretation, and analysis of data across different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22611bda-88c0-49b4-8805-806007cc1807",
   "metadata": {},
   "source": [
    "Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b004d-7eb0-49ba-94d2-e7e55fc57963",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental theorem in probability theory and statistics. It states that the sampling distribution of the sample mean (or sum) of a sufficiently large number of independent and identically distributed random variables will be approximately normally distributed, regardless of the distribution of the original population. In other words, as the sample size increases, the distribution of the sample mean approaches a normal distribution, even if the population distribution is not normal.\n",
    "\n",
    "### Statement of the Central Limit Theorem:\n",
    "Let \\( X_1, X_2, \\ldots, X_n \\) be a sequence of independent and identically distributed (i.i.d.) random variables with mean \\( \\mu \\) and standard deviation \\( \\sigma \\). Then, as \\( n \\) approaches infinity, the distribution of the sample mean \\( \\bar{X} \\) approaches a normal distribution with mean \\( \\mu \\) and standard deviation \\( \\frac{\\sigma}{\\sqrt{n}} \\).\n",
    "\n",
    "### Significance of the Central Limit Theorem:\n",
    "1. **Robustness**: The CLT provides a powerful result that allows statisticians to apply techniques that assume normality to a wide range of situations, even when the underlying distribution is not normal. This makes statistical inference more robust and applicable to real-world data.\n",
    "\n",
    "2. **Inference**: The CLT forms the basis for many statistical inference procedures, such as hypothesis testing, confidence intervals, and parameter estimation. It allows practitioners to make inferences about population parameters using sample data, even when the population distribution is unknown or non-normal.\n",
    "\n",
    "3. **Sampling**: The CLT explains why the sampling distribution of the sample mean tends to be approximately normal, regardless of the shape of the population distribution. This property is crucial for understanding and analyzing data obtained from random samples.\n",
    "\n",
    "4. **Modeling**: The CLT enables researchers to model and analyze complex phenomena using simple and tractable normal distributions. It simplifies the analysis of large datasets and allows for the development of predictive models in various fields, including finance, economics, engineering, and social sciences.\n",
    "\n",
    "5. **Quality Control**: In quality control and manufacturing processes, the CLT is used to monitor and control product quality by analyzing sample means or sample sums. Deviations from expected values can indicate potential issues or defects in the manufacturing process.\n",
    "\n",
    "In summary, the Central Limit Theorem is a fundamental result in statistics with wide-ranging applications in data analysis, inference, modeling, and quality control. It provides a bridge between sample statistics and population parameters, making statistical analysis more robust, interpretable, and applicable to diverse datasets and scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b993aba-8b62-4b16-9b9c-91feff1a80b3",
   "metadata": {},
   "source": [
    "Q10: State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fcb45d-c49a-448b-b129-5bb4d47752c7",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) relies on certain assumptions to hold true. These assumptions are essential for the theorem to be applicable and for the sampling distribution of the sample mean to approximate a normal distribution. The key assumptions of the Central Limit Theorem are:\n",
    "\n",
    "1. **Independence**: The random variables in the sample must be independent of each other. This means that the outcome of one observation should not influence the outcome of another observation. Independence ensures that each observation provides new and unique information about the population.\n",
    "\n",
    "2. **Identical Distribution**: The random variables in the sample must be identically distributed. This means that each observation is drawn from the same population and follows the same probability distribution. The distribution may be any distribution with finite mean and variance.\n",
    "\n",
    "3. **Finite Mean and Variance**: The population from which the random variables are drawn must have a finite mean (\\( \\mu \\)) and a finite variance (\\( \\sigma^2 \\)). The mean and variance of the population determine the mean and variance of the sampling distribution of the sample mean.\n",
    "\n",
    "4. **Sample Size**: While the Central Limit Theorem does not specify an exact sample size requirement, it generally assumes that the sample size (\\( n \\)) is sufficiently large. Although there is no strict cutoff, a common guideline is that the sample size should be greater than or equal to 30. However, in practice, larger sample sizes often lead to better approximations to the normal distribution.\n",
    "\n",
    "5. **Random Sampling**: The sample must be obtained through random sampling. This means that each observation in the sample is chosen randomly from the population, without any systematic bias or preference. Random sampling ensures that the sample is representative of the population.\n",
    "\n",
    "These assumptions are crucial for the Central Limit Theorem to hold true and for the sampling distribution of the sample mean to approximate a normal distribution. Violation of any of these assumptions may lead to unreliable results and invalidate the application of the CLT. Therefore, it is important to carefully consider these assumptions when applying the Central Limit Theorem in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
